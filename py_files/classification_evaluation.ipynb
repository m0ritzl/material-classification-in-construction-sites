{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_evaluation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aEGfKwrFQfB"
      },
      "source": [
        "!pip install kora -q\n",
        "!pip3 install segments-ai -q\n",
        "!pip install efficientnet_pytorch -q\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "import efficientnet_pytorch\n",
        "import neuralnets\n",
        "import dataset_loader\n",
        "from PIL import Image\n",
        "import torch \n",
        "from torchvision import models\n",
        "from torchvision.datasets import CocoDetection\n",
        "import torchvision.transforms as transforms\n",
        "import segments\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Try to use gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io1wNr5IEvJl"
      },
      "source": [
        "First we define a utility class which can load a model withs respect to:\r\n",
        "\r\n",
        "*   Used archictecture ``` arch ```\r\n",
        "*   Dataset used for training ``` dataset ```\r\n",
        "*   Optimizer used during the training ``` optimizer ```\r\n",
        "\r\n",
        "The model should all be placed in the ``` model_root ``` folder.\r\n",
        "For each supported model (``` efficientnet ```, ``` resnet ```) a corresponding folder with the same name needs to be placed in the root folder which results in the following structure: \r\n",
        "```\r\n",
        "path/to/model_root -- efficientnet\r\n",
        "                   |\r\n",
        "                   -- resnet\r\n",
        "```\r\n",
        "The model should be be placed in these folders and be named with this sheme:\r\n",
        "\r\n",
        "``` dataset + \"_\" + optimizer + \".h5\" ```\r\n",
        "\r\n",
        "for example:\r\n",
        "\r\n",
        "``` \"minc_adam.h5\" ```\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wVB2W7VKBhZ"
      },
      "source": [
        "class MaterialClassifier:\n",
        "  @staticmethod\n",
        "  def get_classifier(model_root, arch, dataset, optimizer):\n",
        "    \n",
        "    if dataset == 'minc':\n",
        "      num_classes = 23\n",
        "    else:\n",
        "      num_classes = 4\n",
        "    if arch == 'efficientnet':\n",
        "      model = efficientnet_pytorch.EfficientNet.from_name(\"efficientnet-b1\", num_classes=num_classes)\n",
        "    elif arch == 'resnet':\n",
        "      model = models.wide_resnet50_2(pretrained=False, num_classes=num_classes)\n",
        "    model_dir = os.path.join(model_root, arch)\n",
        "    model_name = dataset + \"_\" + optimizer + \".h5\"\n",
        "    model_path = os.path.join(model_dir, model_name)\n",
        "    state_dict = torch.load(model_path, map_location='cuda:0')#,map_location=torch.device())\n",
        "    ret = model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "    return model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4nqceb8x4Ke"
      },
      "source": [
        "We assume that the segmented data is given in coco format. This way we can utilize the already existing coco loader. (For more details: ``` dataset_loader.ipynb```)\r\n",
        "\r\n",
        "The ```root``` and ```annFile``` need to be set.\r\n",
        "\r\n",
        "Mappings from the MINC-2500 and the OpenSurfaces subset datasets to the ground truth category ids are given by ```minc_to_common``` and ```os_to_common```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvcYdA8Wvo10"
      },
      "source": [
        "root = \"/content/drive/MyDrive/Datasets/segments/computervision2021_Indoor/v2.0.2\"\r\n",
        "annFile = \"/content/drive/MyDrive/Datasets/Indoor-v2.0.2_coco.json\"\r\n",
        "test_dataset = dataset_loader.CocoLoader(root, annFile)\r\n",
        "data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\r\n",
        "                                          batch_size=1, \r\n",
        "                                          shuffle=False)\r\n",
        "minc_to_common = {0: 4, #\"Granite_marble\", \r\n",
        "        1: 7, #\"Concrete\", \r\n",
        "        2: 5, #\"Tile\", \r\n",
        "        3: 3, #\"Wood\"\r\n",
        "        }\r\n",
        "\r\n",
        "os_to_common = {2: 4, #\"Granite_marble\", \r\n",
        "        1: 7, #\"Concrete\", \r\n",
        "        3: 5, #\"Tile\", \r\n",
        "        0: 3, #\"Wood\"\r\n",
        "        }\r\n",
        "        \r\n",
        "minc_selector = [15,18,19,22]\r\n",
        "os_selector = [0,1,2,3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38IqQxHICzH"
      },
      "source": [
        "\r\n",
        "Method to evaluate a model on a dataset. Following parameters needs to be set:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   ``` model ``` - Use ``` MaterialClassifier.get_classifier(...) ``` to get a model\r\n",
        "*   ```data_loader``` - Use ``` data_loader ``` from the last block\r\n",
        "*   ```crop_size``` - Patch size (``` crop_size x crop_size ```) used for classification\r\n",
        "*   ```dataset_to_common``` - Mapping from the dataset category ids to the classifier ids (i.e. use ```minc_to_common``` or ```os_to_common```)\r\n",
        "*   ```num_classes``` - Number of different classes which get estimated by the ``` model ```\r\n",
        "* ```select_classes``` - Classes of intereset (i.e. use ```minc_selector``` or ``` os_selector ```)\r\n",
        "\r\n",
        "The method will return two types of accuracy:\r\n",
        "* Patch accuracy - Ratio of correct patches vs. all patches\r\n",
        "* Segment accuracy - Ratio of correct segments vs. all segments. A segment materials is estimated by a majority vote from all patches inside this segment.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOzYfqpWJLxk"
      },
      "source": [
        "def classify_segments(model, data_loader, crop_size, dataset_to_common, num_classes, select_classes):\n",
        "  trans = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.44074593, 0.44086692, 0.44087528], [0.2285, 0.2284, 0.2285])\n",
        "        ])\n",
        "  common_to_dataset = dict([reversed(i) for i in dataset_to_common.items()])\n",
        "\n",
        "  total_segments = 0\n",
        "  correct_segments = 0\n",
        "  total_patches = 0\n",
        "  correct_patches = 0\n",
        "  with torch.no_grad():\n",
        "    for image, segmentation_mask, name in data_loader:\n",
        "      np_image = np.asarray(image[0])\n",
        "      image = torch.transpose(trans(np_image),0,2)\n",
        "      segmentation_mask = segmentation_mask[0]\n",
        "      height, width = segmentation_mask.size()\n",
        "      if width < crop_size or height < crop_size:\n",
        "        raise ValueError(\"Width of height of image is smaller then crop size\")\n",
        "\n",
        "      sample_count_width = int(width/crop_size)-1\n",
        "      sample_count_height = int(height/crop_size)-1\n",
        "\n",
        "      crop_area = crop_size*crop_size\n",
        "      crop_segment_labels = []\n",
        "      crops = {int(category_id):[] for category_id in torch.unique(segmentation_mask)}\n",
        "\n",
        "      for i in range(sample_count_height):\n",
        "        for j in range(sample_count_width):\n",
        "          crop_segment_label = segmentation_mask[i*crop_size + int(crop_size/2),\n",
        "                                                 j*crop_size + int(crop_size/2)]\n",
        "          if crop_segment_label in [2,3,4,5,6,7]:\n",
        "            left = i * crop_size\n",
        "            right = (i + 1) * crop_size\n",
        "            upper = j * crop_size\n",
        "            lower = (j + 1) * crop_size\n",
        "            crop = image[left:right, upper:lower,:]\n",
        "            crops[int(crop_segment_label)].append(crop)\n",
        "      for category in crops.keys():\n",
        "        patch_count = len(crops[category])\n",
        "        if patch_count > 0:\n",
        "          input_crops = torch.stack(crops[category])\n",
        "          input_crops = torch.transpose(input_crops, 1,3)\n",
        "\n",
        "          images = input_crops.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = outputs.max(1)\n",
        "          category_count = torch.bincount(predicted, minlength=num_classes)[select_classes]\n",
        "\n",
        "          reduced_class = torch.argmax(category_count)\n",
        "          class_of_segment = dataset_to_common[reduced_class.item()]\n",
        "\n",
        "          if class_of_segment == category:\n",
        "            correct_segments += 1\n",
        "          total_segments += 1\n",
        "\n",
        "          total_patches += patch_count\n",
        "          if category in common_to_dataset.keys():\n",
        "            correct_patches += category_count[common_to_dataset[category]].item()\n",
        "\n",
        "  accuracy_segments = correct_segments / total_segments\n",
        "  accuracy_patches = correct_patches / total_patches\n",
        "  return accuracy_segments, accuracy_patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omM3suWtzcfK"
      },
      "source": [
        "The following code will process the tests over 16 combinations of different architectures (```archs```), datasets (```datasets```), optimizers (```optimizers```).\r\n",
        "\r\n",
        "The script expact that all models are places in subdirs of the ```model_dir``` folder.\r\n",
        "You will need one subdir per architecture in ```archs```, named as the architecture. (i.e. ``` /path/to/model_dir/efficientnet ``` or ```/path/to/model_dir/resnet ```).\r\n",
        "\r\n",
        "For each combination the script will print the results like this:\r\n",
        "```\r\n",
        "Architectur: efficientnet, Dataset: os224flipped, Optimizer: adam -- Segment Accuracy: 0.523077, Patch Accuracy: 0.528325\r\n",
        "Architectur: efficientnet, Dataset: os224flipped, Optimizer: sgd -- Segment Accuracy: 0.446154, Patch Accuracy: 0.495746\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Le_7UGaGtR6"
      },
      "source": [
        "archs = [\"efficientnet\" , \"resnet\"]\r\n",
        "datasets = [\"minc\", \"os128\", \"os224\", \"os224flipped\"]\r\n",
        "optimizers = [\"adam\", \"sgd\"]\r\n",
        "model_root = \"/content/drive/MyDrive/models\"\r\n",
        "\r\n",
        "for arch in archs:\r\n",
        "  for dataset in datasets:\r\n",
        "    for optimizer in optimizers:\r\n",
        "      if '224' in dataset:\r\n",
        "        crop_size = 224\r\n",
        "      else:\r\n",
        "        crop_size = 128\r\n",
        "      if dataset == 'minc':\r\n",
        "        num_classes = 23\r\n",
        "        select_classes = minc_selector\r\n",
        "        dataset_to_common = minc_to_common\r\n",
        "      else:\r\n",
        "        num_classes = 4\r\n",
        "        select_classes = os_selector\r\n",
        "        dataset_to_common = os_to_common\r\n",
        "      model = MaterialClassifier.get_classifier(model_root, arch, dataset, optimizer)\r\n",
        "      seg_acc, patch_acc = classify_segments(model, \r\n",
        "                                             data_loader, \r\n",
        "                                             crop_size, \r\n",
        "                                             dataset_to_common, \r\n",
        "                                             num_classes, \r\n",
        "                                             select_classes)\r\n",
        "      print(\"Architectur: %s, Dataset: %s, Optimizer: %s -- Segment Accuracy: %f, Patch Accuracy: %f\" % (arch, dataset, optimizer, seg_acc, patch_acc))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}