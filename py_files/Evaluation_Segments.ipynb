{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation_Segments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-DB1ypMXsiR",
        "outputId": "2e6e4b11-cdc3-419b-9b61-c0dc64b3ad6b"
      },
      "source": [
        "import numpy as np\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import cv2\r\n",
        "from PIL import Image\r\n",
        "from torch.utils.data import Dataset\r\n",
        "# mount drive and setup requirements to import other notebooks\r\n",
        "!pip install kora -q\r\n",
        "from kora import drive\r\n",
        "drive.link_nbs()\r\n",
        "\r\n",
        "#used for calculating the accuracy and the mIoU\r\n",
        "!git init 2>&1 >> install.log\r\n",
        "!git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\r\n",
        "!git pull origin master 2>&1 >> install.log\r\n",
        "\r\n",
        "from mit_semseg.utils import accuracy, intersectionAndUnion\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From https://github.com/CSAILVision/semantic-segmentation-pytorch\n",
            " * branch            master     -> FETCH_HEAD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoQWjZBYx7-K"
      },
      "source": [
        "pretrained semantic segmentation models on ADE20K to perform segmentation on own dataset:\r\n",
        "\r\n",
        "Deeplab V3+ (https://pixellib.readthedocs.io/en/latest/image_ade20k.html\r\n",
        ")\r\n",
        "\r\n",
        "HRNETV2 (https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb)\r\n",
        "\r\n",
        "If you annotate your own dataset with Segments.ai then look at the documentation from Segments.ai on how to export your labeled images (https://docs.segments.ai/export) to coco format to use our evaluation script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMi3qgsxuH_Q"
      },
      "source": [
        "#Dataloader for the labeled images from Segments.ai in coco format\r\n",
        "class CocoLoader(Dataset):\r\n",
        "  def __init__(self, root, annFile, transform = None):\r\n",
        "        from pycocotools.coco import COCO\r\n",
        "        self.root = root\r\n",
        "        self.coco = COCO(annFile)\r\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            index (int): Index\r\n",
        "\r\n",
        "        Returns:\r\n",
        "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\r\n",
        "        \"\"\"\r\n",
        "        coco = self.coco\r\n",
        "        img_id = self.ids[index]\r\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\r\n",
        "        target = coco.loadAnns(ann_ids)\r\n",
        "\r\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\r\n",
        "        #img = Image.open(os.path.join(self.root, path)).convert('RGB')\r\n",
        "        image = cv2.imread(os.path.join(self.root, path))\r\n",
        "        image = np.array(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\r\n",
        "        segmentation_mask = np.ones((image.shape[1], image.shape[0]),dtype=np.uint8)*-1\r\n",
        "        for segmentation in target:\r\n",
        "          binary_mask = coco.annToMask(segmentation)\r\n",
        "          if binary_mask.shape[0] != segmentation_mask.shape[0]:\r\n",
        "            segmentation_mask = segmentation_mask.T\r\n",
        "            image = image.transpose(1,0,2)\r\n",
        "            transposed = True\r\n",
        "          segmentation_mask[binary_mask == 1] = segmentation['category_id']\r\n",
        "          #segmentation_mask = np.maximum(segmentation_mask,\r\n",
        "          #                      coco.annToMask(segmentation)*segmentation['category_id'])\r\n",
        "        output_seg = segmentation_mask.astype(np.uint8)\r\n",
        "        return image, output_seg, path\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "        return len(self.ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvpjl0PZ2Efy"
      },
      "source": [
        "The ```seg_path``` need to be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOjqwKkKeudT"
      },
      "source": [
        "#gt_path = \"/content/drive/MyDrive/Datasets/segments/computervision2021_Indoor/v2.0\"\r\n",
        "seg_path = \"/content/drive/MyDrive/Datasets/segments/computervision2021_Baustelle/v0.2_results_deeplabv3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_g8kuZ2JSV"
      },
      "source": [
        "```ft_mapping``` corresponds to the mapping of the classes in the ground truth.\r\n",
        "\r\n",
        "```pred_mapping``` corresponds to the mapping of the classes with Deeplab V3+.\r\n",
        "\r\n",
        "```hrpred_mapping``` corresponds to the mapping of the classes with HRNETV2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_PPBgYwqIn4"
      },
      "source": [
        "gt_mapping = {-1:-1, # Unkown\r\n",
        "              0:0, # Ceiling\r\n",
        "              1:1, # Wall\r\n",
        "              2:2, # Floor\r\n",
        "              3:2, # Wood (Floor)\r\n",
        "              4:2, # Granite/marble (Floor)\r\n",
        "              5:2, # Tile (Floor)\r\n",
        "              6:2, # unkown_floor (Floor)\r\n",
        "              7:2,} # Concrete (Floor)\r\n",
        "pred_mapping = {-1:-1, # Unkown\r\n",
        "                6:0, #ceiling (Ceiling),\r\n",
        "                1:1, # wall (Wall)\r\n",
        "                4:2, # floor/flooring (Floor)\r\n",
        "                29:2} # rug;carpet;carpeting (Floor)\r\n",
        "                # Any other value needs to be mapped to -1 (Unkown)\r\n",
        "hrpred_mapping = {(120,120,80):0, #ceiling (Ceiling),\r\n",
        "                (120,120,120):1, # wall (Wall)\r\n",
        "                (80,50,50):2, # floor/flooring (Floor)\r\n",
        "                } # rug;carpet;carpeting (Floor)\r\n",
        "                # Any other value needs to be mapped to -1 (Unkown)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjgZXahHAg4p"
      },
      "source": [
        "def gt_to_common(x):\r\n",
        "  return gt_mapping.get(x, -1)\r\n",
        "\r\n",
        "def pred_to_common(x):\r\n",
        "  return pred_mapping.get(x, -1)\r\n",
        "\r\n",
        "def hr_pred_to_common(x, mapping):\r\n",
        "  x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\r\n",
        "  w, h, d = x.shape\r\n",
        "  y = np.ones((w,h)) * -1\r\n",
        "  for c in mapping.keys():\r\n",
        "    y[np.all(x == c,axis=2)] = mapping[c]\r\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSkApsYzBy9k"
      },
      "source": [
        "The ```root``` need to be set to your image directory. \r\n",
        "\r\n",
        "The ```annFile``` need to be set to ```.json``` file from Segments.ai."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEfY1WpShHBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b1c011-0124-409d-cd4e-aa8249d159f1"
      },
      "source": [
        "root = \"/content/drive/MyDrive/Datasets/segments/computervision2021_Baustelle/v1.0\"\r\n",
        "annFile = \"/content/drive/MyDrive/Datasets/Baustelle-v1.0_coco.json\"\r\n",
        "\r\n",
        "loader = torch.utils.data.DataLoader(dataset=CocoLoader(root, annFile),\r\n",
        "                                              batch_size=1, \r\n",
        "                                              num_workers=1,\r\n",
        "                                              shuffle=False)\r\n",
        "\r\n",
        "ious = []\r\n",
        "#for f in os.listdir(seg_path):\r\n",
        "for image, seg, name in loader:\r\n",
        "  name, ext = name[0].split('.')\r\n",
        "  transposed = False\r\n",
        "  gt   = np.array(seg)[0]\r\n",
        "  #pred = np.array(Image.open(os.path.join(seg_path, name + \".png\")))[:,:,0]\r\n",
        "  pred = cv2.imread(os.path.join(seg_path, name + \".png\"))[:,:,0]\r\n",
        "  if transposed:\r\n",
        "    gt = np.flip(gt,1)\r\n",
        "    pred = pred.T\r\n",
        "  #common_pred = hr_pred_to_common(pred, hrpred_mapping)\r\n",
        "  common_pred = np.vectorize(pred_to_common)(pred)\r\n",
        "  common_gt = np.vectorize(gt_to_common)(gt)\r\n",
        "  i,u = intersectionAndUnion(common_pred, common_gt, 3)\r\n",
        "  iou = i/u\r\n",
        "  print(name, iou)\r\n",
        "  #if name == \"DSCF1278\":\r\n",
        "  #  break\r\n",
        "  ious.append(iou)\r\n",
        "\r\n",
        "ious = np.array(ious)\r\n",
        "img_count = ious.shape[0]\r\n",
        "\r\n",
        "ceiling_nans = np.count_nonzero(np.isnan(ious[:,0]))\r\n",
        "wall_nans = np.count_nonzero(np.isnan(ious[:,1]))\r\n",
        "floor_nans = np.count_nonzero(np.isnan(ious[:,2]))\r\n",
        "\r\n",
        "ious[np.isnan(ious)] = 0.0\r\n",
        "\r\n",
        "ceiling_sum, wall_sum, floor_sum = np.sum(ious, axis=0)\r\n",
        "\r\n",
        "ceiling_iou =  100.0 * ceiling_sum / (img_count - ceiling_nans)\r\n",
        "wall_iou =  100.0 * wall_sum / (img_count - wall_nans) \r\n",
        "floor_iou =  100.0 * floor_sum / (img_count - floor_nans)\r\n",
        "\r\n",
        "average_iou = (ceiling_iou + wall_iou + floor_iou) / 3.0\r\n",
        "print(\"Ceiling mIoU: \", ceiling_iou)\r\n",
        "print(\"Wall mIoU: \", wall_iou)\r\n",
        "print(\"Floor mIoU: \", floor_iou)\r\n",
        "print(\"Average mIoU: \", average_iou)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "DSCF1276 [0.         0.70108059 0.        ]\n",
            "DSCF1278 [0.01519578 0.18498333 0.22057426]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DSCF1273 [       nan 0.98280059 0.94639087]\n",
            "DSCF1287 [       nan 0.80182507 0.92691522]\n",
            "DSCF1285 [0.80019679 0.90104093 0.98006373]\n",
            "DSCF1288 [0.95302877 0.88777257 0.81133385]\n",
            "DSCF1291 [       nan 0.9779686  0.95366415]\n",
            "DSCF1290 [       nan 0.97808437 0.81536888]\n",
            "DSCF1293 [       nan 0.7277228  0.21120305]\n",
            "DSCF1294 [0.         0.90897467 0.46490982]\n",
            "DSCF1295 [0.28449926 0.82969282 0.63055381]\n",
            "DSCF1296 [0.09871133 0.31475249 0.60144582]\n",
            "DSCF1299 [0.88733471 0.96734944 0.97934103]\n",
            "DSCF1297 [0.         0.19700392 0.1257235 ]\n",
            "DSCF1300 [0.60468501 0.78983677 0.93921436]\n",
            "DSCF1303 [       nan 0.99216165 0.96270701]\n",
            "IMG_2132 [0.         0.41666065 0.0470093 ]\n",
            "DSCF1304 [0.79953475 0.78810852 0.96468031]\n",
            "IMG_2133 [0.         0.40231495 0.        ]\n",
            "DSCF1307 [0.89391    0.93144296 0.93533764]\n",
            "DSCF1309 [0.94623323 0.98643519 0.92423298]\n",
            "DSCF1310 [0.96907596 0.95808039 0.93627242]\n",
            "Ceiling mIoU:  45.32753492493375\n",
            "Wall mIoU:  75.57315123318288\n",
            "Floor mIoU:  65.3497364034332\n",
            "Average mIoU:  62.08347418718328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MeuSbQ8mX7x",
        "outputId": "6bcca473-e636-47e2-c2a5-72895a153095"
      },
      "source": [
        "ious = a.copy()\r\n",
        "img_count = ious.shape[0]\r\n",
        "\r\n",
        "ceiling_nans = np.count_nonzero(np.isnan(ious[:,0]))\r\n",
        "wall_nans = np.count_nonzero(np.isnan(ious[:,1]))\r\n",
        "floor_nans = np.count_nonzero(np.isnan(ious[:,2]))\r\n",
        "\r\n",
        "ious[np.isnan(ious)] = 0.0\r\n",
        "\r\n",
        "ceiling_sum, wall_sum, floor_sum = np.sum(ious, axis=0)\r\n",
        "\r\n",
        "ceiling_iou =  100.0 * ceiling_sum / (img_count - ceiling_nans)\r\n",
        "wall_iou =  100.0 * wall_sum / (img_count - wall_nans) \r\n",
        "floor_iou =  100.0 * floor_sum / (img_count - floor_nans)\r\n",
        "\r\n",
        "average_iou = (ceiling_iou + wall_iou + floor_iou) / 3.0\r\n",
        "print(\"Ceiling mIoU: \", ceiling_iou)\r\n",
        "print(\"Wall mIoU: \", wall_iou)\r\n",
        "print(\"Floor mIoU: \", floor_iou)\r\n",
        "print(\"Average mIoU: \", average_iou)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ceiling mIoU:  62.627112892482636\n",
            "Wall mIoU:  81.7499708310193\n",
            "Floor mIoU:  88.32411071805382\n",
            "Average mIoU:  77.56706481385191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqloS8JmaSpN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}